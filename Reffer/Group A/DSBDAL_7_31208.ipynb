{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Analytics**\n",
    "1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document Frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.7-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: joblib in /home/pict/.local/lib/python3.8/site-packages (from nltk) (1.1.0)\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.63.1-py2.py3-none-any.whl (76 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 KB\u001b[0m \u001b[31m735.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mMB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2022.3.15-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.9/764.9 KB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, nltk\n",
      "Successfully installed nltk-3.7 regex-2022.3.15 tqdm-4.63.1\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/pict/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/pict/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/pict/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/pict/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package omw-1.4 to /home/pict/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/omw-1.4.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial import distance\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading the data from the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./paragraph.txt') as f:\n",
    "    paragraph = f.read()\n",
    "    paragraph = paragraph.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r jbh pb uywe0pi] uegurv0w \\nfoigb8eytfp u0etu iryue vifhoaiq0yr uifbc78p of\\najd ro o fioj feofh ,hfo  \\n uh ou o; u0 ewi{kpo\\n hvisu biay o oiu8tfijig\\n fuih fojpofj;kd pnkhsdl f;josdhljdoho;\\n dhuicab vfhusrfrihfisbjkzjdghihsp\\\\\\n fd fk jdogihgdjfiuisa fiej \\n fhdsg fi gh l idoufekf \\n aidsi hfyb i iuhihfi i \\n hihih i ifh o klfid fy\\n bfsiv pgpork;gjwiyi'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the first step when working with language tasks, it simplifies the input data by splitting it into sentences or words, as per the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence tokenization\n",
    "sentence_tokens = sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentence tokens : 1\n",
      "Sentence tokens : ['r jbh pb uywe0pi] uegurv0w \\nfoigb8eytfp u0etu iryue vifhoaiq0yr uifbc78p of\\najd ro o fioj feofh ,hfo  \\n uh ou o; u0 ewi{kpo\\n hvisu biay o oiu8tfijig\\n fuih fojpofj;kd pnkhsdl f;josdhljdoho;\\n dhuicab vfhusrfrihfisbjkzjdghihsp\\\\\\n fd fk jdogihgdjfiuisa fiej \\n fhdsg fi gh l idoufekf \\n aidsi hfyb i iuhihfi i \\n hihih i ifh o klfid fy\\n bfsiv pgpork;gjwiyi']\n"
     ]
    }
   ],
   "source": [
    "print('Number of sentence tokens :', len(sentence_tokens))\n",
    "print('Sentence tokens :', sentence_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word tokenization\n",
    "word_tokens = word_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of word tokens : 66\n",
      "Word tokens : ['r', 'jbh', 'pb', 'uywe0pi', ']', 'uegurv0w', 'foigb8eytfp', 'u0etu', 'iryue', 'vifhoaiq0yr', 'uifbc78p', 'of', 'ajd', 'ro', 'o', 'fioj', 'feofh', ',', 'hfo', 'uh', 'ou', 'o', ';', 'u0', 'ewi', '{', 'kpo', 'hvisu', 'biay', 'o', 'oiu8tfijig', 'fuih', 'fojpofj', ';', 'kd', 'pnkhsdl', 'f', ';', 'josdhljdoho', ';', 'dhuicab', 'vfhusrfrihfisbjkzjdghihsp\\\\', 'fd', 'fk', 'jdogihgdjfiuisa', 'fiej', 'fhdsg', 'fi', 'gh', 'l', 'idoufekf', 'aidsi', 'hfyb', 'i', 'iuhihfi', 'i', 'hihih', 'i', 'ifh', 'o', 'klfid', 'fy', 'bfsiv', 'pgpork', ';', 'gjwiyi']\n"
     ]
    }
   ],
   "source": [
    "print('Number of word tokens :', len(word_tokens))\n",
    "print('Word tokens :', word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging and Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop words : {'am', \"should've\", 'were', 'same', 'or', 'that', 'ain', 'for', 't', 'all', 'not', 'mightn', \"she's\", 'about', 'when', 'mustn', 'again', 'she', 'o', 'hadn', 'yours', 'them', 'and', 'wasn', 'ma', \"doesn't\", 'further', 've', \"mightn't\", 'doing', \"don't\", 'nor', 'had', 'after', 'he', 'as', 'at', 'theirs', 'her', 'himself', 'then', 'here', 'herself', 'against', 'their', 'where', 'isn', \"it's\", 'once', 'shouldn', \"mustn't\", 'an', 'more', 'aren', 'yourselves', 'on', 'was', 'it', 'who', 'out', \"haven't\", 'are', 'hasn', 'can', 'if', 'needn', \"isn't\", 'did', 'myself', 'd', 'some', 'itself', \"didn't\", 'most', 'whom', 'wouldn', 'has', 'no', 'own', 'these', 'is', 'a', 'over', 'won', \"wasn't\", 'which', 'me', 'below', 'under', 'couldn', \"weren't\", \"you'd\", 'up', 'the', 'from', 's', 'your', 'to', 'only', 'between', 'have', 'our', 'such', 'each', 'i', \"that'll\", 'during', 'his', 'we', 'having', \"wouldn't\", 'you', 'didn', 'other', \"hasn't\", 'with', 'ourselves', 'those', 'before', 'down', 'll', \"won't\", 'should', 'doesn', 'because', \"you're\", \"aren't\", 'don', 'any', 'hers', 'does', 'm', 'ours', 'y', 'both', \"shouldn't\", 'been', 'above', \"shan't\", 'they', 'of', 'but', 'so', 'just', 'my', 'themselves', 'there', \"hadn't\", 'him', 'until', 'what', 'than', 'while', 'in', 'be', 'being', 'will', 'very', 'off', \"you'll\", 're', \"needn't\", 'how', 'shan', 'now', \"couldn't\", 'this', 'by', 'do', 'through', 'why', 'yourself', 'too', 'few', \"you've\", 'its', 'haven', 'weren', 'into'}\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "print('Stop words :', stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = [word_token for word_token in word_tokens if word_token not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered word tokens : ['r', 'jbh', 'pb', 'uywe0pi', ']', 'uegurv0w', 'foigb8eytfp', 'u0etu', 'iryue', 'vifhoaiq0yr', 'uifbc78p', 'ajd', 'ro', 'fioj', 'feofh', ',', 'hfo', 'uh', 'ou', ';', 'u0', 'ewi', '{', 'kpo', 'hvisu', 'biay', 'oiu8tfijig', 'fuih', 'fojpofj', ';', 'kd', 'pnkhsdl', 'f', ';', 'josdhljdoho', ';', 'dhuicab', 'vfhusrfrihfisbjkzjdghihsp\\\\', 'fd', 'fk', 'jdogihgdjfiuisa', 'fiej', 'fhdsg', 'fi', 'gh', 'l', 'idoufekf', 'aidsi', 'hfyb', 'iuhihfi', 'hihih', 'ifh', 'klfid', 'fy', 'bfsiv', 'pgpork', ';', 'gjwiyi']\n"
     ]
    }
   ],
   "source": [
    "print('Filtered word tokens :', word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: “there is” … think of it like “there exists”) \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective – ‘big’ \n",
    "JJR adjective, comparative – ‘bigger’ \n",
    "JJS adjective, superlative – ‘biggest’ \n",
    "LS list marker 1) \n",
    "MD modal – could, will \n",
    "NN noun, singular ‘- desk’ \n",
    "NNS noun plural – ‘desks’ \n",
    "NNP proper noun, singular – ‘Harrison’ \n",
    "NNPS proper noun, plural – ‘Americans’ \n",
    "PDT predeterminer – ‘all the kids’ \n",
    "POS possessive ending parent’s \n",
    "PRP personal pronoun –  I, he, she \n",
    "PRP$ possessive pronoun – my, his, hers \n",
    "RB adverb – very, silently, \n",
    "RBR adverb, comparative – better \n",
    "RBS adverb, superlative – best \n",
    "RP particle – give up \n",
    "TO – to go ‘to’ the store. \n",
    "UH interjection – errrrrrrrm \n",
    "VB verb, base form – take \n",
    "VBD verb, past tense – took \n",
    "VBG verb, gerund/present participle – taking \n",
    "VBN verb, past participle – taken \n",
    "VBP verb, sing. present, non-3d – take \n",
    "VBZ verb, 3rd person sing. present – takes \n",
    "WDT wh-determiner – which \n",
    "WP wh-pronoun – who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-abverb, eg- where, when\n",
    "'''\n",
    "tagged = nltk.pos_tag(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagged form of filtered word tokens :\n",
      "('r', 'NN')\n",
      "('jbh', 'NN')\n",
      "('pb', 'NN')\n",
      "('uywe0pi', 'JJ')\n",
      "(']', 'NNP')\n",
      "('uegurv0w', 'NN')\n",
      "('foigb8eytfp', 'NN')\n",
      "('u0etu', 'JJ')\n",
      "('iryue', 'NN')\n",
      "('vifhoaiq0yr', 'NN')\n",
      "('uifbc78p', 'JJ')\n",
      "('ajd', 'NN')\n",
      "('ro', 'NN')\n",
      "('fioj', 'NN')\n",
      "('feofh', 'NN')\n",
      "(',', ',')\n",
      "('hfo', 'NN')\n",
      "('uh', 'JJ')\n",
      "('ou', 'NN')\n",
      "(';', ':')\n",
      "('u0', 'JJ')\n",
      "('ewi', 'FW')\n",
      "('{', '(')\n",
      "('kpo', 'VB')\n",
      "('hvisu', 'NN')\n",
      "('biay', 'NN')\n",
      "('oiu8tfijig', 'NN')\n",
      "('fuih', 'NN')\n",
      "('fojpofj', 'NN')\n",
      "(';', ':')\n",
      "('kd', 'CC')\n",
      "('pnkhsdl', 'VB')\n",
      "('f', 'NN')\n",
      "(';', ':')\n",
      "('josdhljdoho', 'NN')\n",
      "(';', ':')\n",
      "('dhuicab', 'CC')\n",
      "('vfhusrfrihfisbjkzjdghihsp\\\\', 'FW')\n",
      "('fd', 'JJ')\n",
      "('fk', 'NN')\n",
      "('jdogihgdjfiuisa', 'NN')\n",
      "('fiej', 'NN')\n",
      "('fhdsg', 'NN')\n",
      "('fi', 'NN')\n",
      "('gh', 'NN')\n",
      "('l', 'NN')\n",
      "('idoufekf', 'NN')\n",
      "('aidsi', 'NN')\n",
      "('hfyb', 'NN')\n",
      "('iuhihfi', 'NN')\n",
      "('hihih', 'NN')\n",
      "('ifh', 'NN')\n",
      "('klfid', 'NN')\n",
      "('fy', 'NN')\n",
      "('bfsiv', 'NN')\n",
      "('pgpork', 'NN')\n",
      "(';', ':')\n",
      "('gjwiyi', 'NN')\n"
     ]
    }
   ],
   "source": [
    "print('POS Tagged form of filtered word tokens :')\n",
    "for tag in tagged:\n",
    "    print(tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Stemming\n",
      "r --> r\n",
      "jbh --> jbh\n",
      "pb --> pb\n",
      "uywe0pi --> uywe0pi\n",
      "] --> ]\n",
      "uegurv0w --> uegurv0w\n",
      "foigb8eytfp --> foigb8eytfp\n",
      "u0etu --> u0etu\n",
      "iryue --> iryu\n",
      "vifhoaiq0yr --> vifhoaiq0yr\n",
      "uifbc78p --> uifbc78p\n",
      "ajd --> ajd\n",
      "ro --> ro\n",
      "fioj --> fioj\n",
      "feofh --> feofh\n",
      ", --> ,\n",
      "hfo --> hfo\n",
      "uh --> uh\n",
      "ou --> ou\n",
      "; --> ;\n",
      "u0 --> u0\n",
      "ewi --> ewi\n",
      "{ --> {\n",
      "kpo --> kpo\n",
      "hvisu --> hvisu\n",
      "biay --> biay\n",
      "oiu8tfijig --> oiu8tfijig\n",
      "fuih --> fuih\n",
      "fojpofj --> fojpofj\n",
      "kd --> kd\n",
      "pnkhsdl --> pnkhsdl\n",
      "f --> f\n",
      "josdhljdoho --> josdhljdoho\n",
      "dhuicab --> dhuicab\n",
      "vfhusrfrihfisbjkzjdghihsp\\ --> vfhusrfrihfisbjkzjdghihsp\\\n",
      "fd --> fd\n",
      "fk --> fk\n",
      "jdogihgdjfiuisa --> jdogihgdjfiuisa\n",
      "fiej --> fiej\n",
      "fhdsg --> fhdsg\n",
      "fi --> fi\n",
      "gh --> gh\n",
      "l --> l\n",
      "idoufekf --> idoufekf\n",
      "aidsi --> aidsi\n",
      "hfyb --> hfyb\n",
      "iuhihfi --> iuhihfi\n",
      "hihih --> hihih\n",
      "ifh --> ifh\n",
      "klfid --> klfid\n",
      "fy --> fy\n",
      "bfsiv --> bfsiv\n",
      "pgpork --> pgpork\n",
      "gjwiyi --> gjwiyi\n"
     ]
    }
   ],
   "source": [
    "print('Results of Stemming')\n",
    "stemmed = {word: ps.stem(word) for word in word_tokens}\n",
    "for pair in stemmed.items():\n",
    "    print('{0} --> {1}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results of Lemmatization\n",
      "r --> r\n",
      "jbh --> jbh\n",
      "pb --> pb\n",
      "uywe0pi --> uywe0pi\n",
      "] --> ]\n",
      "uegurv0w --> uegurv0w\n",
      "foigb8eytfp --> foigb8eytfp\n",
      "u0etu --> u0etu\n",
      "iryue --> iryue\n",
      "vifhoaiq0yr --> vifhoaiq0yr\n",
      "uifbc78p --> uifbc78p\n",
      "ajd --> ajd\n",
      "ro --> ro\n",
      "fioj --> fioj\n",
      "feofh --> feofh\n",
      ", --> ,\n",
      "hfo --> hfo\n",
      "uh --> uh\n",
      "ou --> ou\n",
      "; --> ;\n",
      "u0 --> u0\n",
      "ewi --> ewi\n",
      "{ --> {\n",
      "kpo --> kpo\n",
      "hvisu --> hvisu\n",
      "biay --> biay\n",
      "oiu8tfijig --> oiu8tfijig\n",
      "fuih --> fuih\n",
      "fojpofj --> fojpofj\n",
      "kd --> kd\n",
      "pnkhsdl --> pnkhsdl\n",
      "f --> f\n",
      "josdhljdoho --> josdhljdoho\n",
      "dhuicab --> dhuicab\n",
      "vfhusrfrihfisbjkzjdghihsp\\ --> vfhusrfrihfisbjkzjdghihsp\\\n",
      "fd --> fd\n",
      "fk --> fk\n",
      "jdogihgdjfiuisa --> jdogihgdjfiuisa\n",
      "fiej --> fiej\n",
      "fhdsg --> fhdsg\n",
      "fi --> fi\n",
      "gh --> gh\n",
      "l --> l\n",
      "idoufekf --> idoufekf\n",
      "aidsi --> aidsi\n",
      "hfyb --> hfyb\n",
      "iuhihfi --> iuhihfi\n",
      "hihih --> hihih\n",
      "ifh --> ifh\n",
      "klfid --> klfid\n",
      "fy --> fy\n",
      "bfsiv --> bfsiv\n",
      "pgpork --> pgpork\n",
      "gjwiyi --> gjwiyi\n"
     ]
    }
   ],
   "source": [
    "print('Results of Lemmatization')\n",
    "lemmatized = {word: lemmatizer.lemmatize(word) for word in word_tokens}\n",
    "for pair in lemmatized.items():\n",
    "    print('{0} --> {1}'.format(pair[0], pair[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Term-Frequency and Inverse Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_convert_1d(arr):\n",
    "    arr = np.array(arr)\n",
    "    arr = np.concatenate( arr, axis=0 )\n",
    "    arr = np.concatenate( arr, axis=0 )\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = []\n",
    "def cosine(trans):\n",
    "    cos.append(cosine_similarity(trans[0], trans[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "manhatten = []\n",
    "def manhatten_distance(trans):\n",
    "    manhatten.append(pairwise_distances(trans[0], trans[1], metric = 'manhattan'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "euclidean = []\n",
    "def euclidean_function(vectors):\n",
    "    euc=euclidean_distances(vectors[0], vectors[1])\n",
    "    euclidean.append(euc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(str1, str2):\n",
    "    vect = TfidfVectorizer()\n",
    "    vect.fit(word_tokens)\n",
    "    corpus = [str1,str2]\n",
    "    trans = vect.transform(corpus)\n",
    "    euclidean_function(trans)\n",
    "    cosine(trans)\n",
    "    manhatten_distance(trans)\n",
    "    return convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert():\n",
    "    dataf = pd.DataFrame()\n",
    "    lis2 = arr_convert_1d(manhatten)\n",
    "    dataf['manhatten'] = lis2\n",
    "    lis2 = arr_convert_1d(cos)\n",
    "    dataf['cos_sim'] = lis2\n",
    "    lis2 = arr_convert_1d(euclidean)\n",
    "    dataf['euclidean'] = lis2\n",
    "    return dataf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   manhatten  cos_sim  euclidean\n",
      "0        0.0      0.0        0.0\n",
      "1        0.0      0.0        0.0\n"
     ]
    }
   ],
   "source": [
    "str1 = 'rsfhcui'\n",
    "str2 = 'ukjiorgd'\n",
    "newData = tfidf(str1,str2);\n",
    "print(newData);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
